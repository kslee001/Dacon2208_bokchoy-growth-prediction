{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bccb97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.jit as jit\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm as tq\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "'''functions'''\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+torch.exp(-x))\n",
    "\n",
    "def clip_grad(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += torch.sum(grad ** 2)\n",
    "    total_norm = torch.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "            \n",
    "def load_batch(X, Y, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates batches with the remainder dropped.\n",
    "\n",
    "    Do NOT modify this function\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        X = X[permutation, :]\n",
    "        Y = Y[permutation, :]\n",
    "    num_steps = int(X.shape[0])//batch_size\n",
    "    step = 0\n",
    "    while step<num_steps:\n",
    "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
    "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
    "        step+=1\n",
    "        yield X_batch, Y_batch\n",
    "\n",
    "# def to_gpu(x):\n",
    "#     import cupy\n",
    "#     if type(x) == cupy.ndarray:\n",
    "#         return x\n",
    "#     return cupy.asarray(x)\n",
    "\n",
    "'''loss function'''\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.cache  = None\n",
    "        self.loss   = None\n",
    "\n",
    "    def forward(self, yhat, ygt):\n",
    "        self.cache = yhat-ygt\n",
    "        self.loss  = torch.sum(torch.sqrt(self.cache**2))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dyhat = dout * 2 * self.cache\n",
    "        return dyhat # (N, H)    \n",
    "    \n",
    "    \n",
    "# class RMSELoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.mse = nn.MSELoss()\n",
    "        \n",
    "#     def forward(self,yhat,y):\n",
    "#         return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "    \n",
    "'''optimizer'''\n",
    "class custom_SGD:\n",
    "    def __init__(self, lr=0.01, clip=True, max_norm=10):\n",
    "        self.lr = lr\n",
    "        self.clip = clip\n",
    "        self.max_norm = max_norm\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.clip:\n",
    "            clip_grad(grads, self.max_norm)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b341f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, W, b, device):\n",
    "        '''\n",
    "        shapes\n",
    "        x : (N, I)  /  W : (I, O)  /  b : (O, )\n",
    "        '''\n",
    "        self.device = device\n",
    "        self.params = [W.to(device), b.to(device)]\n",
    "        self.grads  = [\n",
    "            torch.zeros_like(W).to(device),\n",
    "            torch.zeros_like(b).to(device)\n",
    "        ]\n",
    "        self.x   = None\n",
    "        self.out = None\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        W, b = self.params[0].to(device), self.params[1].to(device)\n",
    "        self.x = x\n",
    "        self.out = (x @ W + b).to(device)\n",
    "        return self.out\n",
    "\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        shapes - dout : (N, O)\n",
    "        dx : (N, I)  /  dW : (I, O)  /  db : (O, )\n",
    "        '''\n",
    "        dout = dout.to(device)\n",
    "        W, b = self.params[0].to(device), self.params[1].to(device)\n",
    "        dW = (self.x.T @ dout).to(device)\n",
    "        db = torch.sum(dout, axis=0).to(device)\n",
    "        dx = (dout @ W.T).to(device)\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, Wx, Wh, b, device):\n",
    "        self.device = device\n",
    "        self.params = [Wx.to(self.device), Wh.to(self.device), b.to(self.device)]\n",
    "        self.grads  = [\n",
    "            torch.zeros_like(Wx).to(device),\n",
    "            torch.zeros_like(Wh).to(device),\n",
    "            torch.zeros_like(b).to(device)\n",
    "        ]\n",
    "        \n",
    "        self.cache = None\n",
    "    \n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        global f,g,i,A, x_copy ,Wx\n",
    "        Wx, Wh, b = self.params\n",
    "        if x.ndim==1:\n",
    "            x = torch.reshape(x, newshape=(1, x.size))\n",
    "        x = x.to(device)\n",
    "        x_copy = x\n",
    "        Wx_copy = Wx\n",
    "        h_prev = h_prev.to(device)\n",
    "        c_prev = c_prev.to(device)\n",
    "        \n",
    "        N, H = h_prev.shape\n",
    "        A = ((x@Wx) + (h_prev@Wh) + b).to(device)\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = torch.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i,f,g,o,c_next = self.cache # computed in previous time step\n",
    "\n",
    "        tanh_c_next  = torch.tanh(c_next)\n",
    "        dtanh_c_next = dh_next * o\n",
    "        do           = dh_next * tanh_c_next * o * (1-o)\n",
    "        \n",
    "        dsum = dc_next + dtanh_c_next*(1-tanh_c_next**2)\n",
    "\n",
    "        dc_prev = dsum*f\n",
    "        df = dsum * c_prev * f * (1-f)\n",
    "        dg = dsum * i * (1-g**2)\n",
    "        di = dsum * g * i * (1-i)\n",
    "\n",
    "        dA  = torch.hstack((df,dg,di,do)).to(device)\n",
    "        dWh = h_prev.T @ dA\n",
    "        dWx = x.T @ dA\n",
    "        db  = torch.sum(dA, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx      = dA @ Wx.T # (N, 4H) @ (4H, I) -> (N, I)\n",
    "        dh_prev = dA @ Wh.T\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "        \n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b, device, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [\n",
    "            torch.zeros_like(Wx),\n",
    "            torch.zeros_like(Wh),\n",
    "            torch.zeros_like(b)\n",
    "        ]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.hs = None\n",
    "        self.dh = None\n",
    "        self.T  = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        self.T  = T\n",
    "        H = Wh.shape[0]\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = torch.empty(size=(N,T,H))\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            h = torch.zeros(size=(N,H))\n",
    "        if not self.stateful or self.c is None:\n",
    "            c = torch.zeros(size=(N,H))\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = LSTMCell(*self.params, device)\n",
    "            h, c = layer.forward(xs[:, t, :], h, c)\n",
    "            hs[:, t, :] = h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.hs = hs\n",
    "        return self.hs\n",
    "\n",
    "    \n",
    "    def backward(self, dh_final): # loss from another layer\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H      = dh_final.shape\n",
    "        I         = Wx.shape[0]\n",
    "        T         = self.T\n",
    "\n",
    "        dxs = torch.empty(size=(N,T,I))\n",
    "        dh_cur = dh_final\n",
    "        dh_prev, dc = 0,0\n",
    "\n",
    "        grads = [0,0,0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh_prev, dc = layer.backward(dh_cur, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            dh_cur = dh_prev + dh_cur\n",
    "\n",
    "            for idx, grad in enumerate(layer.grads):\n",
    "                grads[idx] += grad\n",
    "        \n",
    "        for idx, grad in enumerate(grads):\n",
    "            self.grads[idx][...] = grad\n",
    "\n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc00f5c",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cff964",
   "metadata": {},
   "source": [
    "- timestep이 1440이나 되는데, 굳이 1분단위로 끊어서 볼 이유가 없다!\n",
    "  - 1시간 단위로 time step 끊어볼 것\n",
    "  - seasonality 처리\n",
    "- variable 정리 (feature engineering)\n",
    "  - 겹치는 variables 하나로 합치기 (ex. \n",
    "  - 불필요한 variables 제거 (외부온도, 습도 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e123ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9801287211c14843a8e5461ff498c866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83c62ecf70347db937912de7aec0aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_shape : torch.Size([1607, 1440, 37]) | Y_train_shape : torch.Size([1607, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['시간', '내부온도관측치', '내부습도관측치', 'CO2관측치', 'EC관측치', '외부온도관측치', '외부습도관측치',\n",
       "       '펌프상태', '펌프작동남은시간', '최근분무량', '일간누적분무량', '냉방상태', '냉방작동남은시간', '난방상태',\n",
       "       '난방작동남은시간', '내부유동팬상태', '내부유동팬작동남은시간', '외부환기팬상태', '외부환기팬작동남은시간',\n",
       "       '화이트 LED상태', '화이트 LED작동남은시간', '화이트 LED동작강도', '레드 LED상태', '레드 LED작동남은시간',\n",
       "       '레드 LED동작강도', '블루 LED상태', '블루 LED작동남은시간', '블루 LED동작강도', '카메라상태', '냉방온도',\n",
       "       '난방온도', '기준온도', '난방부하', '냉방부하', '총추정광량', '백색광추정광량', '적색광추정광량',\n",
       "       '청색광추정광량'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing(X_input, Y_input, X_container, Y_container):    \n",
    "    y_maxlen = 0\n",
    "    for x,y in tq(zip(X_input, Y_input)):\n",
    "        curx = pd.read_csv(x).drop(columns = [\"시간\"]).fillna(0).values\n",
    "        x_len = len(curx)//1440\n",
    "        x_temp = []\n",
    "        for idx in range(x_len):\n",
    "            x_temp.append(curx[1440*idx : 1440*(idx+1)])\n",
    "        x_temp = torch.Tensor(x_temp)\n",
    "        X_container.append(x_temp)\n",
    "        y_temp = torch.Tensor(pd.read_csv(y)[\"rate\"].fillna(0).values)\n",
    "        y_temp = y_temp.reshape(y_temp.size()[0], 1)\n",
    "        Y_container.append(y_temp)\n",
    "    return;\n",
    "\n",
    "all_input_list  = sorted(glob.glob(\"train_input/*.csv\"))\n",
    "all_target_list = sorted(glob.glob(\"train_target/*.csv\"))\n",
    "# for training\n",
    "train_input_list = all_input_list[:50]\n",
    "train_target_list = all_target_list[:50]\n",
    "# for validation\n",
    "test_input_list = all_input_list[50:]\n",
    "test_target_list = all_target_list[50:]\n",
    "\n",
    "X_train = []; Y_train = []\n",
    "X_val  = []; Y_val  = []\n",
    "\n",
    "# call function\n",
    "preprocessing(train_input_list, train_target_list, X_train, Y_train)\n",
    "preprocessing(test_input_list, test_target_list, X_val, Y_val)\n",
    "\n",
    "# stack X, Y data\n",
    "X_train = torch.vstack(X_train)\n",
    "Y_train = torch.vstack(Y_train)\n",
    "X_val  = torch.vstack(X_val)\n",
    "Y_val  = np.vstack(Y_val)\n",
    "\n",
    "winter = pd.read_csv(train_input_list[28])\n",
    "summer = pd.read_csv(train_input_list[19])\n",
    "print(f\"X_train_shape : {X_train.shape} | Y_train_shape : {Y_train.shape}\")\n",
    "winter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37547f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    \"시간\", \"내부온도관측치\", \"내부습도관측치\", \"CO2\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15c6b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>시간</th>\n",
       "      <th>내부온도관측치</th>\n",
       "      <th>내부습도관측치</th>\n",
       "      <th>CO2관측치</th>\n",
       "      <th>EC관측치</th>\n",
       "      <th>외부온도관측치</th>\n",
       "      <th>외부습도관측치</th>\n",
       "      <th>펌프상태</th>\n",
       "      <th>펌프작동남은시간</th>\n",
       "      <th>최근분무량</th>\n",
       "      <th>...</th>\n",
       "      <th>카메라상태</th>\n",
       "      <th>냉방온도</th>\n",
       "      <th>난방온도</th>\n",
       "      <th>기준온도</th>\n",
       "      <th>난방부하</th>\n",
       "      <th>냉방부하</th>\n",
       "      <th>총추정광량</th>\n",
       "      <th>백색광추정광량</th>\n",
       "      <th>적색광추정광량</th>\n",
       "      <th>청색광추정광량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-27 00:00:00</td>\n",
       "      <td>24.1</td>\n",
       "      <td>65.300003</td>\n",
       "      <td>811.0</td>\n",
       "      <td>0.698426</td>\n",
       "      <td>24.660000</td>\n",
       "      <td>34.720001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.500002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-27 00:01:00</td>\n",
       "      <td>24.1</td>\n",
       "      <td>65.400002</td>\n",
       "      <td>808.0</td>\n",
       "      <td>0.699099</td>\n",
       "      <td>24.940000</td>\n",
       "      <td>34.480002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.500002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-27 00:02:00</td>\n",
       "      <td>24.1</td>\n",
       "      <td>65.400002</td>\n",
       "      <td>810.0</td>\n",
       "      <td>0.698426</td>\n",
       "      <td>25.280000</td>\n",
       "      <td>33.820000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.500002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-27 00:03:00</td>\n",
       "      <td>24.1</td>\n",
       "      <td>65.400002</td>\n",
       "      <td>815.0</td>\n",
       "      <td>0.698426</td>\n",
       "      <td>25.580000</td>\n",
       "      <td>33.340001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.500002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-27 00:04:00</td>\n",
       "      <td>24.1</td>\n",
       "      <td>65.400002</td>\n",
       "      <td>807.0</td>\n",
       "      <td>0.699099</td>\n",
       "      <td>25.859999</td>\n",
       "      <td>33.040001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.500002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    시간  내부온도관측치    내부습도관측치  CO2관측치     EC관측치    외부온도관측치  \\\n",
       "0  2022-01-27 00:00:00     24.1  65.300003   811.0  0.698426  24.660000   \n",
       "1  2022-01-27 00:01:00     24.1  65.400002   808.0  0.699099  24.940000   \n",
       "2  2022-01-27 00:02:00     24.1  65.400002   810.0  0.698426  25.280000   \n",
       "3  2022-01-27 00:03:00     24.1  65.400002   815.0  0.698426  25.580000   \n",
       "4  2022-01-27 00:04:00     24.1  65.400002   807.0  0.699099  25.859999   \n",
       "\n",
       "     외부습도관측치  펌프상태  펌프작동남은시간  최근분무량  ...  카메라상태  냉방온도  난방온도  기준온도  난방부하  \\\n",
       "0  34.720001     0         0    0.0  ...      0  16.0  14.0  15.0   0.0   \n",
       "1  34.480002     0         0    0.0  ...      0  16.0  14.0  15.0   0.0   \n",
       "2  33.820000     0         0    0.0  ...      0  16.0  14.0  15.0   0.0   \n",
       "3  33.340001     0         0    0.0  ...      0  16.0  14.0  15.0   0.0   \n",
       "4  33.040001     0         0    0.0  ...      0  16.0  14.0  15.0   0.0   \n",
       "\n",
       "        냉방부하  총추정광량  백색광추정광량  적색광추정광량  청색광추정광량  \n",
       "0  50.500002    0.0      0.0      0.0      0.0  \n",
       "1  50.500002    0.0      0.0      0.0      0.0  \n",
       "2  50.500002    0.0      0.0      0.0      0.0  \n",
       "3  50.500002    0.0      0.0      0.0      0.0  \n",
       "4  50.500002    0.0      0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winter.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5314535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mymodel():\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, optimizer, device):\n",
    "        self.I = input_dim\n",
    "        self.H = hidden_dim\n",
    "        self.O = output_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # initialization\n",
    "        lstm_Wx = torch.Tensor(self.I, 4*self.H).uniform_(-1/np.sqrt(self.I), 1/np.sqrt(self.I))\n",
    "        lstm_Wh = torch.Tensor(self.H, 4*self.H).uniform_(-1/np.sqrt(self.H), 1/np.sqrt(self.H))\n",
    "        lstm_b = torch.zeros(size=(4 * self.H,))\n",
    "        \n",
    "        linear_W = torch.Tensor(self.H, self.O).uniform_(-1/np.sqrt(self.H), 1/np.sqrt(self.H))\n",
    "        linear_b = torch.zeros(size=(self.O,))\n",
    "        \n",
    "        # layers\n",
    "        self.layers = [\n",
    "            LSTM(lstm_Wx, lstm_Wh, lstm_b, device=self.device, stateful=True),\n",
    "            Linear(linear_W, linear_b, self.device)\n",
    "        ]\n",
    "        self.loss_layer = MSELoss()\n",
    "        self.lstm   = self.layers[0]\n",
    "        self.linear = self.layers[1]\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        self.yhat = None\n",
    "        self.xs = None\n",
    "\n",
    "    def predict(self, xs):\n",
    "        self.xs = xs\n",
    "        lstm_output = self.lstm.forward(self.xs)\n",
    "        lstm_yhat   = lstm_output[:, -1, :] # last hidden state        \n",
    "        yhat = self.linear.forward(lstm_yhat)\n",
    "        return yhat\n",
    "\n",
    "    def forward(self, xs, ygt):\n",
    "        xs = xs.to(self.device)\n",
    "        ygt = ygt.to(self.device)\n",
    "        self.xs = xs.to(self.device)\n",
    "        self.yhat = self.predict(self.xs).to(self.device)\n",
    "        loss = self.loss_layer.forward(self.yhat, ygt).to(self.device)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dyhat    = self.loss_layer.backward().to(self.device)\n",
    "        dh_final = self.linear.backward(dyhat).to(self.device)\n",
    "        self.lstm.backward(dh_final)\n",
    "        return\n",
    "    \n",
    "    def update(self):\n",
    "        for layer in reversed(self.layers):\n",
    "            self.optimizer.update(layer.params, layer.grads)\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_val, Y_val,lr, n_epochs, batch_size):        \n",
    "        for epoch in tq(range(n_epochs)):\n",
    "            train_loss = 0.0\n",
    "            len_batch  = 0\n",
    "            for X_batch, Y_batch in load_batch(X_train, Y_train, batch_size):\n",
    "                len_batch +=1\n",
    "                train_loss += self.__step(X_batch, Y_batch)\n",
    "            train_loss /= len_batch\n",
    "            print(f\"EPOCH {epoch+1} train loss : {train_loss:.4f}\")\n",
    "                        \n",
    "                \n",
    "    def __step(self, X_batch, Y_batch):\n",
    "        # forward step\n",
    "        loss = self.forward(X_batch, Y_batch)\n",
    "        # backward step\n",
    "        self.backward()\n",
    "        # update\n",
    "        self.update()\n",
    "        return loss\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "865ad7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device : cuda\n",
      "X_train shape : torch.Size([1607, 1440, 37]) | Y_train shape :torch.Size([1607, 1])\n"
     ]
    }
   ],
   "source": [
    "'''parameters'''\n",
    "learning_rate = 0.007\n",
    "n_epochs = 30\n",
    "batch_size = 64\n",
    "max_norm = 10.0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"current device : {device}\")\n",
    "\n",
    "print(f\"X_train shape : {X_train.shape} | Y_train shape :{Y_train.shape}\")\n",
    "\n",
    "N = batch_size\n",
    "T = X_train.shape[1]\n",
    "I = X_train.shape[2]\n",
    "H = 256\n",
    "O = Y_train.shape[1]\n",
    "\n",
    "\n",
    "'''model setting'''\n",
    "optim = custom_SGD(lr=learning_rate, clip=True, max_norm=max_norm)\n",
    "\n",
    "model = mymodel(\n",
    "    input_dim=I,\n",
    "    hidden_dim=H,\n",
    "    output_dim=O,\n",
    "    optimizer=optim,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3947b2",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f553f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2399eacd101545da8291dc0b0947c8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 train loss : 24.3252\n",
      "EPOCH 2 train loss : 18.2241\n",
      "EPOCH 3 train loss : 18.5976\n",
      "EPOCH 4 train loss : 17.6601\n",
      "EPOCH 5 train loss : 19.3855\n",
      "EPOCH 6 train loss : 16.9328\n",
      "EPOCH 7 train loss : 17.9755\n",
      "EPOCH 8 train loss : 17.9139\n",
      "EPOCH 9 train loss : 17.9045\n",
      "EPOCH 10 train loss : 17.7653\n",
      "EPOCH 11 train loss : 18.2333\n",
      "EPOCH 12 train loss : 18.8732\n",
      "EPOCH 13 train loss : 18.1337\n",
      "EPOCH 14 train loss : 18.3313\n",
      "EPOCH 15 train loss : 18.2875\n",
      "EPOCH 16 train loss : 18.5967\n",
      "EPOCH 17 train loss : 18.6147\n",
      "EPOCH 18 train loss : 17.8838\n",
      "EPOCH 19 train loss : 18.9357\n",
      "EPOCH 20 train loss : 18.5086\n",
      "EPOCH 21 train loss : 17.8950\n",
      "EPOCH 22 train loss : 18.2739\n",
      "EPOCH 23 train loss : 18.9428\n",
      "EPOCH 24 train loss : 19.5383\n",
      "EPOCH 25 train loss : 18.5862\n",
      "EPOCH 26 train loss : 18.9273\n",
      "EPOCH 27 train loss : 18.6373\n",
      "EPOCH 28 train loss : 18.3074\n",
      "EPOCH 29 train loss : 18.9113\n",
      "EPOCH 30 train loss : 17.8283\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    X_train=X_train[:400],\n",
    "    Y_train=Y_train[:400],\n",
    "    X_val=X_val,\n",
    "    Y_val=Y_val,\n",
    "    lr=learning_rate,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=N\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc878cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tch",
   "language": "python",
   "name": "tch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
