{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c289ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f\n",
    "from torch.nn.utils import clip_grad_norm_ as clip_grad\n",
    "\n",
    "from tqdm.auto import tqdm as tq\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52526cba",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(X, Y, batch_size, shuffle=True):\n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        X = X[permutation, :]\n",
    "        Y = Y[permutation, :]\n",
    "    num_steps = int(X.shape[0])//batch_size\n",
    "    step = 0\n",
    "    while step<num_steps:\n",
    "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
    "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
    "        step+=1\n",
    "        yield X_batch, Y_batch\n",
    "        \n",
    "        \n",
    "def valid(model, device, optimizer, criterion, batch_size, X_val, Y_val):\n",
    "    # mode change\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    num = 0\n",
    "    # avoid unnecessary calculations\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in load_batch(X_val, Y_val, batch_size):\n",
    "            num += batch_size\n",
    "            # forward\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            yhat = model(X_batch)\n",
    "                \n",
    "            # loss\n",
    "            loss = criterion(yhat, Y_batch)\n",
    "            \n",
    "            # save loss values\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    return np.sum(val_loss)/num\n",
    "\n",
    "\n",
    "def train(model, device, criterion, optimizer, scheduler, clip, X_train, Y_train, X_val, Y_val, lr, n_epochs, batch_size, max_norm):\n",
    "    model.to(device)\n",
    "    best_loss  = 999999999999\n",
    "    best_model = None\n",
    "    \n",
    "    \n",
    "    for epoch in tq(range(1, n_epochs+1)):\n",
    "        num = 0\n",
    "        train_loss = []\n",
    "        \n",
    "        # mode change\n",
    "        model.train()\n",
    "        for X_batch, Y_batch in load_batch(X_train, Y_train, batch_size):\n",
    "            num+= batch_size\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)            \n",
    "            yhat = model(X_batch)\n",
    "            \n",
    "            # loss\n",
    "            loss = criterion(yhat, Y_batch)\n",
    "            \n",
    "            # backward\n",
    "            loss.backward()\n",
    "            if clip :\n",
    "                clip_grad(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # save loss values\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "        val_loss = valid(model, device, optimizer, criterion, batch_size, X_val, Y_val)\n",
    "        print(f'Train Loss : [{np.sum(train_loss)/num:.5f}] Valid Loss : [{val_loss:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            if epoch > 20:\n",
    "                scheduler.step()\n",
    "            \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "#             print(\" -- best model found -- \")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "'''Loss function'''\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, yhat, ygt):\n",
    "        return torch.sqrt(self.mse(yhat, ygt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faccdfd",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6591c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_original(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: torch.Tensor) -> torch.Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: torch.Tensor) -> torch.Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return tensors[0] + self.dropout(self.norm(self.sublayer(*tensors)))\n",
    "    \n",
    "    \n",
    "def feed_forward(dim_input, dim_feedforward) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.GELU(), # activation function of original model was relu \n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    ")\n",
    "\n",
    "\n",
    "def position_encoding(seq_len, dim_model, \n",
    "                      device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")) -> torch.Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim // dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e5984",
   "metadata": {},
   "source": [
    "### Attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84bbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query:torch.Tensor, key:torch.Tensor, value:torch.Tensor)->torch.Tensor:  # value -> value\n",
    "    # temp : Q @ K.T   -> (N.T.D) @ (N,D,T)  (batch size만 그대로 두고, D,T 와 T, D를 )\n",
    "    # Q @ K.T -> (N, T, T)\n",
    "    QKT     = query.bmm(key.transpose(1, 2))  # bmm : batch matrix multiplication (X, O, O)-> O에 해당되는 dim에 대해서만 matmul 진행\n",
    "    root_dk = query.size(-1)**0.5            # squared root of D\n",
    "    softmax = f.softmax( QKT / root_dk, dim= -1 ) # softmax for \"T of Key\", not for \"T of Query\", so dim = -1 is right\n",
    "                                                  # dim = -2 로 맞추면 Key 에 대한 쿼리 결과(세로축)으로 1을 합산하는 꼴임\n",
    "    return softmax.bmm(value) # (N,T,T)@(N,T,D) -> (N, T, D)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module): # X = (N, D)  / Wq, Wk, Wv : (D, Q) or (D, K) / 일반적으로 K, Q는 같은 dimension 사용\n",
    "    def __init__(self, input_dim:int, query_dim:int, key_dim:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.q_linear = nn.Linear(input_dim, query_dim) # generate Q \n",
    "        self.k_linear = nn.Linear(input_dim, key_dim) # generate K\n",
    "        self.v_linaer = nn.Linear(input_dim, key_dim) # generate V\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "        # 인풋으로 들어온 query, key, value 텐서에 대해 linear forward를 진행하고\n",
    "        # 그 과정을 통해 만들어진 Q, K, V를 그대로 scaled_dot_product_attention 에 forward 시킴        \n",
    "        return scaled_dot_product_attention(\n",
    "            self.q_linear(query), # query @ q_linear : (Xq : N, D) @ (Wq : D, Q) -> (N, Q)\n",
    "            self.k_linear(key),   # key   @ k_linear : (Xk : N, D) @ (Wk : D, K) -> (N, K)\n",
    "            self.v_linaer(value)) # value @ v_linear : (Xv : N, D) @ (Wv : D, K) -> (N, K)\n",
    "    \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, input_dim: int, query_dim: int, key_dim: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(input_dim, query_dim, key_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * key_dim, input_dim)  \n",
    "        # num_heads 만큼 horizontally concat 되므로, \n",
    "        # multiheadAttention의 forward 결과 나오는 concated V의 dimension 은 numm_heads배 늘어난다. \n",
    "        # 즉, (N, T, K) * num_heads -> (N, T, num_heads * K)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([ head(query, key, value) for head in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d55a65",
   "metadata": {},
   "source": [
    "### TransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a57b124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads, dim_model, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        dim_q = max(dim_model // num_heads, 1)\n",
    "        dim_k = dim_q\n",
    "        \n",
    "        self.attention = MultiHeadAttention(\n",
    "                num_heads = num_heads,\n",
    "                input_dim = dim_model,\n",
    "                query_dim = dim_q, \n",
    "                key_dim   = dim_k\n",
    "        )\n",
    "        \n",
    "        self.residual_AT = Residual(\n",
    "            sublayer  = self.attention,\n",
    "            dimension = dim_model,\n",
    "            dropout   = dropout\n",
    "        )\n",
    "            \n",
    "        self.feed_forward = feed_forward(\n",
    "            dim_input       = dim_model,\n",
    "            dim_feedforward = dim_feedforward\n",
    "        )\n",
    "            \n",
    "        self.residual_FF = Residual(\n",
    "            sublayer = self.feed_forward,\n",
    "            dimension = dim_model,\n",
    "            dropout   = dropout,\n",
    "        )\n",
    "\n",
    "    ''' source shape : (N, T, D)'''\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.residual_AT(x, x, x)\n",
    "        return self.residual_FF(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_layers, \n",
    "                 num_heads, \n",
    "                 dim_model, \n",
    "                 dim_feedforward, \n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(\n",
    "                    num_heads = num_heads, \n",
    "                    dim_model = dim_model, \n",
    "                    dim_feedforward = dim_feedforward, \n",
    "                    dropout = dropout\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # shape\n",
    "        if(x.ndim==2):\n",
    "            x = x.reshape(1, x.size(0), x.size(1))\n",
    "        N, T, D = x.shape\n",
    "        \n",
    "        # positional encoding\n",
    "        x += position_encoding(T, D)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a27a03",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7012d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_columns = ['시간', '내부온도관측치', '내부습도관측치', 'CO2관측치', 'EC관측치', '외부온도관측치', '외부습도관측치',\n",
    "       '펌프상태', '펌프작동남은시간', '최근분무량', '일간누적분무량', '냉방상태', '냉방작동남은시간', '난방상태',\n",
    "       '난방작동남은시간', '내부유동팬상태', '내부유동팬작동남은시간', '외부환기팬상태', '외부환기팬작동남은시간',\n",
    "       '화이트 LED상태', '화이트 LED작동남은시간', '화이트 LED동작강도', '레드 LED상태', '레드 LED작동남은시간',\n",
    "       '레드 LED동작강도', '블루 LED상태', '블루 LED작동남은시간', '블루 LED동작강도', '카메라상태', '냉방온도',\n",
    "       '난방온도', '기준온도', '난방부하', '냉방부하', '총추정광량', '백색광추정광량', '적색광추정광량',\n",
    "       '청색광추정광량']\n",
    "\n",
    "target_columns = [\n",
    "    '내부온도관측치',\n",
    "    '내부습도관측치',\n",
    "    'CO2관측치',    \n",
    "    'EC관측치',\n",
    "    '외부온도관측치',\n",
    "    '외부습도관측치',\n",
    "    '최근분무량',\n",
    "    '일간누적분무량',\n",
    "    '화이트 LED동작강도',\n",
    "    '레드 LED동작강도',\n",
    "    '블루 LED동작강도',\n",
    "    '냉방온도',\n",
    "    '난방온도',\n",
    "    '기준온도',\n",
    "    '난방부하',\n",
    "    '냉방부하', \n",
    "    '백색광추정광량',\n",
    "    '적색광추정광량',\n",
    "    '청색광추정광량',\n",
    "    '월'\n",
    "]\n",
    "\n",
    "def preprocessing(X_input, Y_input, X_container, Y_container):     \n",
    "    drop_list = ['시간',\n",
    "        '펌프상태','펌프작동남은시간', \n",
    "     '냉방상태', '냉방작동남은시간', '난방상태',\n",
    "           '난방작동남은시간', '내부유동팬상태', '내부유동팬작동남은시간', '외부환기팬상태', '외부환기팬작동남은시간',\n",
    "           '화이트 LED상태', '화이트 LED작동남은시간','레드 LED상태','레드 LED작동남은시간', '블루 LED상태','블루 LED작동남은시간',  '카메라상태', '총추정광량', \n",
    "    ]\n",
    "    for x,y in tq(zip(X_input, Y_input)):\n",
    "        curx = pd.read_csv(x)\n",
    "        curx.columns = origin_columns\n",
    "        curx[\"월\"] = curx[\"시간\"].str.split(\"-\", expand = True)[1].astype(int)\n",
    "        curx[\"월\"] = np.where(\n",
    "            curx[\"월\"]==12, 0, curx[\"월\"] \n",
    "        )\n",
    "        try:\n",
    "            curx = curx[target_columns].fillna(0).values\n",
    "        except:\n",
    "            curx = curx[target_columns2].fillna(0).values\n",
    "        x_len = len(curx)//1440\n",
    "        x_temp = []\n",
    "        for idx in range(x_len):\n",
    "            x_temp.append(curx[1440*idx : 1440*(idx+1)])\n",
    "        x_temp = torch.Tensor(x_temp)\n",
    "        X_container.append(x_temp)\n",
    "        y_temp = torch.Tensor(pd.read_csv(y)[\"rate\"].fillna(0).values)\n",
    "        y_temp = y_temp.reshape(y_temp.size()[0], 1)\n",
    "        Y_container.append(y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c65b78",
   "metadata": {},
   "source": [
    "### 계절을 고려하여 dataset을 배분 -> X  | 고려하지 않고 계절 정보를 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf1b172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e927d66a00f74860a1ea45260935c563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of processed X and Y  : \n",
      "torch.Size([1813, 1440, 20])\n",
      "torch.Size([1813, 1])\n"
     ]
    }
   ],
   "source": [
    "# data random shuffling\n",
    "all_input_list  = sorted(glob.glob(\"train_input/*.csv\"))\n",
    "all_target_list = sorted(glob.glob(\"train_target/*.csv\"))\n",
    "all_data = [ (all_input_list[i], all_target_list[i]) for i in range(len(all_input_list)) ]\n",
    "\n",
    "processed_X = []\n",
    "processed_Y = []\n",
    "\n",
    "# run preprocessing function\n",
    "preprocessing(all_input_list, all_target_list, processed_X, processed_Y)\n",
    "processed_X = torch.vstack(processed_X)\n",
    "processed_Y = torch.vstack(processed_Y)\n",
    "\n",
    "print(\"shapes of processed X and Y  : \")\n",
    "print(processed_X.shape)\n",
    "print(processed_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681c28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, X_val_all, Y_train_all, Y_val_all = train_test_split(processed_X, processed_Y, test_size = 0.15, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee64fe",
   "metadata": {},
   "source": [
    "### Define model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25497205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device : cuda\n",
      "lr             : 0.001\n",
      "n_epochs       : 100\n",
      "max_norm       : 10\n",
      "\n",
      "N (batch_size)  : 64\n",
      "T (time_step)   : 1440\n",
      "I (input_dim)   : 20\n",
      "H (hidden_dim)  : 2048\n",
      "O (output_dim)  : 1\n"
     ]
    }
   ],
   "source": [
    "class origin(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(origin, self).__init__()\n",
    "        \n",
    "        self.attention_forward = TransformerEncoder(\n",
    "            num_layers = 6,\n",
    "            num_heads  = 6,\n",
    "            dim_model  = input_dim,\n",
    "            dim_feedforward = hidden_dim,\n",
    "            dropout = 0.1\n",
    "        )\n",
    "        \n",
    "#         self.attention_backward = TransformerEncoder(\n",
    "#             num_layers = 4,\n",
    "#             num_heads  = 6,\n",
    "#             dim_model  = input_dim,\n",
    "#             dim_feedforward = hidden_dim,\n",
    "#             dropout = 0.12\n",
    "#         )\n",
    "            \n",
    "        self.linear = nn.Linear(\n",
    "            in_features  = input_dim,\n",
    "            out_features = output_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        forward_x  = self.attention_forward(x)\n",
    "#         backward_x = self.attention_backward(x.flip(dims=[1]))\n",
    "#         con = forward_x + backward_x    \n",
    "        x = self.linear(forward_x)[:, -1, :]\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "'''parameters'''\n",
    "lr = 0.001\n",
    "n_epochs = 100\n",
    "max_norm = 10\n",
    "N = 32   # batch_size\n",
    "T = processed_X.shape[1]\n",
    "I = processed_X.shape[2]\n",
    "O = processed_Y.shape[1]  \n",
    "H = 2048\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"current device : {device}\")\n",
    "print(f\"lr             : {lr}\")\n",
    "print(f\"n_epochs       : {n_epochs}\")\n",
    "print(f\"max_norm       : {max_norm}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"N (batch_size)  : {N}\")\n",
    "print(f\"T (time_step)   : {T}\")\n",
    "print(f\"I (input_dim)   : {I}\")\n",
    "print(f\"H (hidden_dim)  : {H}\")\n",
    "print(f\"O (output_dim)  : {O}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b2c3d",
   "metadata": {},
   "source": [
    "### free gpu cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a7d92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999aab01",
   "metadata": {},
   "source": [
    "### Generate, train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1bfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = origin(I, H, O)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(), \n",
    "    lr = lr,\n",
    "    eps = 1e-08,\n",
    "    weight_decay = 0.1\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "clip      = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99ad19b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c464d26a6c04a8c84b64057bcd6537a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 23.70 GiB total capacity; 22.18 GiB already allocated; 318.56 MiB free; 22.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_train_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_val_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_norm\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_final_AB2_state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_final_AB2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, criterion, optimizer, scheduler, clip, X_train, Y_train, X_val, Y_val, lr, n_epochs, batch_size, max_norm)\u001b[0m\n\u001b[1;32m     57\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m Y_batch \u001b[38;5;241m=\u001b[39m Y_batch\u001b[38;5;241m.\u001b[39mto(device)            \n\u001b[0;32m---> 59\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m     62\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(yhat, Y_batch)\n",
      "File \u001b[0;32m~/anaconda/envs/tch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36morigin.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):        \n\u001b[0;32m---> 27\u001b[0m         forward_x  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#         backward_x = self.attention_backward(x.flip(dims=[1]))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#         con = forward_x + backward_x    \u001b[39;00m\n\u001b[1;32m     30\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(forward_x)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/anaconda/envs/tch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_encoding(T, D)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda/envs/tch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_AT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_FF(x)\n",
      "File \u001b[0;32m~/anaconda/envs/tch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mResidual.forward\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Assume that the \"query\" tensor is given first, so we can compute the\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# residual.  This matches the signature of 'MultiHeadAttention'.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensors[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/anaconda/envs/tch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: torch\u001b[38;5;241m.\u001b[39mTensor, key: torch\u001b[38;5;241m.\u001b[39mTensor, value: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\n\u001b[0;32m---> 41\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat([ head(query, key, value) \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     )\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: torch\u001b[38;5;241m.\u001b[39mTensor, key: torch\u001b[38;5;241m.\u001b[39mTensor, value: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\n\u001b[0;32m---> 41\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat([ \u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda/envs/tch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mAttentionHead.forward\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: torch\u001b[38;5;241m.\u001b[39mTensor, key: torch\u001b[38;5;241m.\u001b[39mTensor, value: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# 인풋으로 들어온 query, key, value 텐서에 대해 linear forward를 진행하고\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# 그 과정을 통해 만들어진 Q, K, V를 그대로 scaled_dot_product_attention 에 forward 시킴        \u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# query @ q_linear : (Xq : N, D) @ (Wq : D, Q) -> (N, Q)\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# key   @ k_linear : (Xk : N, D) @ (Wk : D, K) -> (N, K)\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_linaer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(query, key, value)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscaled_dot_product_attention\u001b[39m(query:torch\u001b[38;5;241m.\u001b[39mTensor, key:torch\u001b[38;5;241m.\u001b[39mTensor, value:torch\u001b[38;5;241m.\u001b[39mTensor)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor:  \u001b[38;5;66;03m# value -> value\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# temp : Q @ K.T   -> (N.T.D) @ (N,D,T)  (batch size만 그대로 두고, D,T 와 T, D를 )\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Q @ K.T -> (N, T, T)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     QKT     \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# bmm : batch matrix multiplication (X, O, O)-> O에 해당되는 dim에 대해서만 matmul 진행\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     root_dk \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m            \u001b[38;5;66;03m# squared root of D\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     softmax \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39msoftmax( QKT \u001b[38;5;241m/\u001b[39m root_dk, dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m ) \u001b[38;5;66;03m# softmax for \"T of Key\", not for \"T of Query\", so dim = -1 is right\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 23.70 GiB total capacity; 22.18 GiB already allocated; 318.56 MiB free; 22.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "best_model = train(\n",
    "    model, \n",
    "    device, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    clip, \n",
    "    X_train_all, \n",
    "    Y_train_all, \n",
    "    X_val_all, \n",
    "    Y_val_all, \n",
    "    lr, \n",
    "    n_epochs, \n",
    "    N,\n",
    "    max_norm\n",
    ")\n",
    "torch.save(best_model.state_dict(), \"best_model_final_AB2_state\")\n",
    "torch.save(best_model, \"best_model_final_AB2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec6bbd",
   "metadata": {},
   "source": [
    "### test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a54db",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx= np.random.randint(0, 16)\n",
    "tempx = X_val_all[idx:(idx+10)]\n",
    "tempy = Y_val_all[idx:(idx+10)]\n",
    "out = best_model.forward(tempx.to(device))\n",
    "for i in range(10):\n",
    "    print( f\"{out[i].item():.4f}  |  {tempy[i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba3a5e",
   "metadata": {},
   "source": [
    "### result labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.read_csv(\"test_target/TEST_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac536eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tch",
   "language": "python",
   "name": "tch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
