{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae547142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cupy as np\n",
    "import random\n",
    "import glob\n",
    "\n",
    "from tqdm.auto import tqdm as tq\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd41b1",
   "metadata": {},
   "source": [
    "### functions and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a64c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''functions'''\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def clip_grad(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "            \n",
    "def load_batch(X, Y, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates batches with the remainder dropped.\n",
    "\n",
    "    Do NOT modify this function\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        X = X[permutation, :]\n",
    "        Y = Y[permutation, :]\n",
    "    num_steps = int(X.shape[0])//batch_size\n",
    "    step = 0\n",
    "    while step<num_steps:\n",
    "        X_batch = X[batch_size*step:batch_size*(step+1)]\n",
    "        Y_batch = Y[batch_size*step:batch_size*(step+1)]\n",
    "        step+=1\n",
    "        yield X_batch, Y_batch\n",
    "\n",
    "def to_gpu(x):\n",
    "    import cupy\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.asarray(x)\n",
    "\n",
    "'''loss function'''\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.cache  = None\n",
    "        self.loss   = None\n",
    "\n",
    "    def forward(self, yhat, ygt):\n",
    "        self.cache = yhat-ygt\n",
    "        self.loss  = np.sum(np.sqrt(self.cache**2))\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dyhat = dout * 2 * self.cache\n",
    "        return dyhat # (N, H)    \n",
    "\n",
    "'''optimizer'''\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01, clip=True, max_norm=10):\n",
    "        self.lr = lr\n",
    "        self.clip = clip\n",
    "        self.max_norm = max_norm\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.clip:\n",
    "            clip_grad(grads, self.max_norm)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999,\n",
    "                clip=True, max_norm=10,\n",
    "                scheduler = False):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.clip = clip\n",
    "        self.max_norm = max_norm\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.clip:\n",
    "            clip_grad(grads, self.max_norm)\n",
    "\n",
    "            \n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ccfe7",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a9eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, W, b):\n",
    "        '''\n",
    "        shapes\n",
    "        x : (N, I)  /  W : (I, O)  /  b : (O, )\n",
    "        '''\n",
    "        self.params = [W, b]\n",
    "        self.grads  = [\n",
    "            np.zeros_like(W),\n",
    "            np.zeros_like(b)\n",
    "        ]\n",
    "        self.x   = None\n",
    "        self.out = None\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        self.x = x\n",
    "        self.out = x @ W + b\n",
    "        return self.out\n",
    "\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        shapes - dout : (N, O)\n",
    "        dx : (N, I)  /  dW : (I, O)  /  db : (O, )\n",
    "        '''\n",
    "        W, b = self.params\n",
    "        dW = self.x.T @ dout\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dx = dout @ W.T\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads  = [\n",
    "            np.zeros_like(Wx),\n",
    "            np.zeros_like(Wh),\n",
    "            np.zeros_like(b)\n",
    "        ]\n",
    "        self.cache = None\n",
    "    \n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        global f,g,i,A\n",
    "        Wx, Wh, b = self.params\n",
    "        if x.ndim==1:\n",
    "            x = np.reshape(x, newshape=(1, x.size))\n",
    "        N, H = h_prev.shape\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i,f,g,o,c_next = self.cache # computed in previous time step\n",
    "\n",
    "        tanh_c_next  = np.tanh(c_next)\n",
    "        dtanh_c_next = dh_next * o\n",
    "        do           = dh_next * tanh_c_next * o * (1-o)\n",
    "        \n",
    "        dsum = dc_next + dtanh_c_next*(1-tanh_c_next**2)\n",
    "\n",
    "        dc_prev = dsum*f\n",
    "        df = dsum * c_prev * f * (1-f)\n",
    "        dg = dsum * i * (1-g**2)\n",
    "        di = dsum * g * i * (1-i)\n",
    "\n",
    "        dA  = np.hstack((df,dg,di,do))\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T     , dA)\n",
    "        db  = np.sum(dA, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx      = np.dot(dA, Wx.T) # (N, 4H) @ (4H, I) -> (N, I)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "        \n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [\n",
    "            np.zeros_like(Wx),\n",
    "            np.zeros_like(Wh),\n",
    "            np.zeros_like(b)\n",
    "        ]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.hs = None\n",
    "        self.dh = None\n",
    "        self.T  = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        self.T  = T\n",
    "        H = Wh.shape[0]\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty(shape=(N,T,H), dtype = 'f')\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            h = np.zeros(shape=(N,H), dtype = 'f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            c = np.zeros(shape=(N,H), dtype = 'f')\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = LSTMCell(*self.params)\n",
    "            h, c = layer.forward(xs[:, t, :], h, c)\n",
    "            hs[:, t, :] = h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.hs = hs\n",
    "        return self.hs\n",
    "\n",
    "    \n",
    "    def backward(self, dh_final): # loss from another layer\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H      = dh_final.shape\n",
    "        I         = Wx.shape[0]\n",
    "        T         = self.T\n",
    "\n",
    "        dxs = np.empty(shape=(N,T,I), dtype = 'f')\n",
    "        dh_cur = dh_final\n",
    "        dh_prev, dc = 0,0\n",
    "\n",
    "        grads = [0,0,0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh_prev, dc = layer.backward(dh_cur, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            dh_cur = dh_prev + dh_cur\n",
    "\n",
    "            for idx, grad in enumerate(layer.grads):\n",
    "                grads[idx] += grad\n",
    "        \n",
    "        for idx, grad in enumerate(grads):\n",
    "            self.grads[idx][...] = grad\n",
    "\n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc00f5c",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cff964",
   "metadata": {},
   "source": [
    "- timestep이 1440이나 되는데, 굳이 1분단위로 끊어서 볼 이유가 없다!\n",
    "  - 1시간 단위로 time step 끊어볼 것\n",
    "  - seasonality 처리\n",
    "- variable 정리 (feature engineering)\n",
    "  - 겹치는 variables 하나로 합치기 (ex. \n",
    "  - 불필요한 variables 제거 (외부온도, 습도 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2e7aa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4405249461f4222a836a8163bd09958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b25252e67f64333bee3a111bf3452f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocessing(X_input, Y_input, X_container, Y_container):    \n",
    "    for x,y in tq(zip(X_input, Y_input)):\n",
    "        curx = pd.read_csv(x).drop(columns = [\"시간\"]).fillna(0).values\n",
    "        x_len = len(curx)//1440\n",
    "        x_temp = []\n",
    "        for idx in range(x_len):\n",
    "            x_temp.append(curx[1440*idx : 1440*(idx+1)])\n",
    "        x_temp = np.array(x_temp)\n",
    "        X_container.append(x_temp)\n",
    "        y_temp = pd.read_csv(y)[\"rate\"].fillna(0).values\n",
    "        y_temp = y_temp.reshape(y_temp.size, 1)\n",
    "        Y_container.append(y_temp)\n",
    "    return;\n",
    "\n",
    "all_input_list  = sorted(glob.glob(\"train_input/*.csv\"))\n",
    "all_target_list = sorted(glob.glob(\"train_target/*.csv\"))\n",
    "# for training\n",
    "train_input_list = all_input_list[:50]\n",
    "train_target_list = all_target_list[:50]\n",
    "# for validation\n",
    "test_input_list = all_input_list[50:]\n",
    "test_target_list = all_target_list[50:]\n",
    "\n",
    "X_train = []; Y_train = []\n",
    "X_val  = []; Y_val  = []\n",
    "\n",
    "# call function\n",
    "preprocessing(train_input_list, train_target_list, X_train, Y_train)\n",
    "preprocessing(test_input_list, test_target_list, X_val, Y_val)\n",
    "\n",
    "# stack X, Y data\n",
    "X_train = np.vstack(X_train)\n",
    "Y_train = np.vstack(Y_train)\n",
    "X_val  = np.vstack(X_val)\n",
    "Y_val  = np.vstack(Y_val)\n",
    "\n",
    "winter = pd.read_csv(train_input_list[28])\n",
    "summer = pd.read_csv(train_input_list[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b78fc34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['시간', '내부온도관측치', '내부습도관측치', 'CO2관측치', 'EC관측치', '외부온도관측치', '외부습도관측치',\n",
       "       '펌프상태', '펌프작동남은시간', '최근분무량', '일간누적분무량', '냉방상태', '냉방작동남은시간', '난방상태',\n",
       "       '난방작동남은시간', '내부유동팬상태', '내부유동팬작동남은시간', '외부환기팬상태', '외부환기팬작동남은시간',\n",
       "       '화이트 LED상태', '화이트 LED작동남은시간', '화이트 LED동작강도', '레드 LED상태', '레드 LED작동남은시간',\n",
       "       '레드 LED동작강도', '블루 LED상태', '블루 LED작동남은시간', '블루 LED동작강도', '카메라상태', '냉방온도',\n",
       "       '난방온도', '기준온도', '난방부하', '냉방부하', '총추정광량', '백색광추정광량', '적색광추정광량',\n",
       "       '청색광추정광량'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6bcab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mymodel():\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, optimizer=SGD()):\n",
    "        self.I = input_dim\n",
    "        self.H = hidden_dim\n",
    "        self.O = output_dim\n",
    "        \n",
    "        # initialization\n",
    "        lstm_Wx = np.random.uniform(-1/np.sqrt(self.I), 1/np.sqrt(self.I), (self.I, 4*self.H)).astype('f')\n",
    "        lstm_Wh = np.random.uniform(-1/np.sqrt(self.H), 1/np.sqrt(self.H), (self.H, 4*self.H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * self.H).astype('f')\n",
    "        \n",
    "        linear_W = np.random.uniform(-1/np.sqrt(self.H), 1/np.sqrt(self.H), (self.H, self.O)).astype('f')\n",
    "        linear_b = np.zeros(self.O).astype('f')\n",
    "        \n",
    "        # layers\n",
    "        self.layers = [\n",
    "            LSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            Linear(linear_W, linear_b)\n",
    "        ]\n",
    "        self.loss_layer = MSELoss()\n",
    "        self.lstm   = self.layers[0]\n",
    "        self.linear = self.layers[1]\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        self.yhat = None\n",
    "        self.xs = None\n",
    "\n",
    "    def predict(self, xs):\n",
    "        self.xs = xs\n",
    "        lstm_output = self.lstm.forward(self.xs)\n",
    "        lstm_yhat   = lstm_output[:, -1, :] # last hidden state        \n",
    "        yhat = self.linear.forward(lstm_yhat)\n",
    "        return yhat\n",
    "\n",
    "    def forward(self, xs, ygt):\n",
    "        xs = to_gpu(xs)\n",
    "        ygt = to_gpu(ygt)\n",
    "        self.xs = xs\n",
    "        self.yhat = to_gpu(self.predict(self.xs))\n",
    "        loss = to_gpu(self.loss_layer.forward(self.yhat, ygt))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        global dyhat, dh_final\n",
    "        dyhat    = self.loss_layer.backward()\n",
    "        dh_final = self.linear.backward(dyhat)\n",
    "        self.lstm.backward(dh_final)\n",
    "        return\n",
    "    \n",
    "    def update(self):\n",
    "        for layer in reversed(self.layers):\n",
    "            self.optimizer.update(layer.params, layer.grads)\n",
    "    \n",
    "            \n",
    "    def train(self, X_train, Y_train, X_val, Y_val,lr, n_epochs, batch_size):        \n",
    "        for epoch in tq(range(n_epochs)):\n",
    "            train_loss = 0.0\n",
    "            len_batch  = 0\n",
    "            for X_batch, Y_batch in load_batch(X_train, Y_train, batch_size):\n",
    "                len_batch +=1\n",
    "                train_loss += self.__step(X_batch, Y_batch)\n",
    "            train_loss /= len_batch\n",
    "            print(f\"EPOCH {epoch+1} train loss : {train_loss}:.4f\")\n",
    "                        \n",
    "                \n",
    "    def __step(self, X_batch, Y_batch):\n",
    "        # forward step\n",
    "        loss = self.forward(X_batch, Y_batch)\n",
    "        # backward step\n",
    "        self.backward()\n",
    "        # update\n",
    "        self.update()\n",
    "        return loss\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92951481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (1607, 1440, 37) | Y_train shape :(1607, 1)\n"
     ]
    }
   ],
   "source": [
    "'''parameters'''\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "max_norm = 10.0\n",
    "\n",
    "print(f\"X_train shape : {X_train.shape} | Y_train shape :{Y_train.shape}\")\n",
    "\n",
    "N = batch_size\n",
    "T = X_train.shape[1]\n",
    "I = X_train.shape[2]\n",
    "H = 256\n",
    "O = Y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c38bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 train loss : 15.472272297050205\n",
      "EPOCH 2 train loss : 12.576168322150162\n",
      "EPOCH 3 train loss : 11.983155631756334\n",
      "EPOCH 4 train loss : 11.28772398782278\n",
      "EPOCH 5 train loss : 11.032550114557097\n",
      "EPOCH 6 train loss : 10.878009968652973\n",
      "EPOCH 7 train loss : 10.612003333582132\n",
      "EPOCH 8 train loss : 10.696159005365667\n",
      "EPOCH 9 train loss : 10.560528295462527\n",
      "EPOCH 10 train loss : 10.367403541679877\n",
      "EPOCH 11 train loss : 10.064469600002466\n",
      "EPOCH 12 train loss : 9.777436484413444\n",
      "EPOCH 13 train loss : 10.161913441980978\n",
      "EPOCH 14 train loss : 10.003887779671253\n",
      "EPOCH 15 train loss : 9.986247578555643\n",
      "EPOCH 16 train loss : 9.46685390726328\n",
      "EPOCH 17 train loss : 9.85066124708593\n",
      "EPOCH 18 train loss : 9.889478909557363\n",
      "EPOCH 19 train loss : 9.863484480485567\n",
      "EPOCH 20 train loss : 10.485375983440777\n"
     ]
    }
   ],
   "source": [
    "'''model setting'''\n",
    "optim = SGD(lr=learning_rate, clip=True, max_norm=max_norm)\n",
    "\n",
    "model = mymodel(\n",
    "    input_dim=I,\n",
    "    hidden_dim=H,\n",
    "    output_dim=O,\n",
    "    optimizer = optim)\n",
    "\n",
    "model.train(\n",
    "    X_train=X_train[:400],\n",
    "    Y_train=Y_train[:400],\n",
    "    X_val=X_val,\n",
    "    Y_val=Y_val,\n",
    "    lr=learning_rate,\n",
    "    n_epochs=n_epochs,\n",
    "    batch_size=N\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ec9ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50233746]\n",
      " [0.46261957]\n",
      " [0.293731  ]\n",
      " [0.15924153]\n",
      " [0.2746481 ]\n",
      " [0.35048905]\n",
      " [0.26311737]\n",
      " [0.2422494 ]\n",
      " [0.4629143 ]\n",
      " [0.45377657]\n",
      " [0.3794228 ]\n",
      " [0.28812164]\n",
      " [0.42093238]\n",
      " [0.4188284 ]\n",
      " [0.47035125]\n",
      " [0.5388825 ]\n",
      " [0.52928525]\n",
      " [0.27833822]\n",
      " [0.16517842]\n",
      " [0.32458213]]\n",
      "[[ 0.71429]\n",
      " [ 0.36111]\n",
      " [ 0.28571]\n",
      " [ 0.26984]\n",
      " [ 0.4    ]\n",
      " [ 0.375  ]\n",
      " [ 0.48701]\n",
      " [ 0.43231]\n",
      " [ 0.48476]\n",
      " [ 0.35113]\n",
      " [ 0.3541 ]\n",
      " [ 0.46801]\n",
      " [ 0.29128]\n",
      " [ 0.35642]\n",
      " [ 0.26757]\n",
      " [ 0.1918 ]\n",
      " [ 0.11846]\n",
      " [ 0.05425]\n",
      " [ 0.18549]\n",
      " [-0.01013]]\n"
     ]
    }
   ],
   "source": [
    "tempx = X_val[80:100]\n",
    "tempy = Y_val[80:100]\n",
    "print(model.predict(tempx))\n",
    "print(tempy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70e6969b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26415396]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tempx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60aa6733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tch",
   "language": "python",
   "name": "tch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
